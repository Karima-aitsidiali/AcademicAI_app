# Guide Complet : SystÃ¨me RAG et Analyse de Sentiment

## Table des MatiÃ¨res
1. [Introduction au SystÃ¨me](#introduction)
2. [Architecture GÃ©nÃ©rale](#architecture)
3. [SystÃ¨me RAG (Retrieval Augmented Generation)](#rag)
4. [Analyse de Sentiment](#sentiment)
5. [IntÃ©gration et Workflow Complet](#workflow)

---

## 1. Introduction au SystÃ¨me {#introduction}

Votre application AcaBot est un **chatbot Ã©ducatif intelligent** qui utilise deux technologies avancÃ©es pour offrir une expÃ©rience d'apprentissage personnalisÃ©e :

- **RAG (Retrieval Augmented Generation)** : Pour rÃ©cupÃ©rer et gÃ©nÃ©rer des rÃ©ponses basÃ©es sur des documents pÃ©dagogiques
- **Analyse de Sentiment** : Pour analyser les retours des utilisateurs et amÃ©liorer l'expÃ©rience

Cette application permet aux Ã©tudiants de poser des questions sur leurs cours et de recevoir des rÃ©ponses prÃ©cises basÃ©es sur les documents de leur formation.

---

## 2. Architecture GÃ©nÃ©rale {#architecture}

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documents     â”‚â”€â”€â”€â–¶â”‚   SystÃ¨me RAG    â”‚â”€â”€â”€â–¶â”‚   RÃ©ponses      â”‚
â”‚   PÃ©dagogiques  â”‚    â”‚   (FAISS+LLM)    â”‚    â”‚   GÃ©nÃ©rÃ©es      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Feedbacks     â”‚â”€â”€â”€â–¶â”‚   Analyse de     â”‚â”€â”€â”€â–¶â”‚   AmÃ©lioration  â”‚
â”‚   Utilisateurs  â”‚    â”‚   Sentiment      â”‚    â”‚   Continue      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. SystÃ¨me RAG (Retrieval Augmented Generation) {#rag}

### 3.1 Qu'est-ce que le RAG ?

Le RAG est une technique qui **combine la recherche d'informations avec la gÃ©nÃ©ration de texte**. Au lieu de se fier uniquement Ã  la mÃ©moire d'un modÃ¨le de langage, le RAG :

1. **Recherche** des passages pertinents dans une base de donnÃ©es
2. **Utilise** ces passages comme contexte pour gÃ©nÃ©rer une rÃ©ponse

### 3.2 Code Complet de la Classe RAGChatbot

ğŸ“ **Fichier : `rag_chatbot.py`**

Voici le code complet de votre systÃ¨me RAG avec explications dÃ©taillÃ©es :

```python
import sys
import time
import os, re
import pickle
import faiss
import numpy as np
from colorama import Fore, Style
from sentence_transformers import SentenceTransformer

class RAGChatbot:
    def __init__(self, ollama_api, chunk_size=384, chunk_overlap=96, 
                 faiss_index_file='./vector_store/faiss_index.faiss', 
                 metadata_file='./vector_store/metadata.pickle', 
                 hashes_file='./vector_store/hashes.pickle'):
        """
        Initialise le chatbot RAG avec tous ses composants.
        
        Ligne 1: ollama_api = Interface pour communiquer avec le modÃ¨le de langage
        Ligne 2: embedding_model = ModÃ¨le pour convertir le texte en vecteurs numÃ©riques
        Ligne 3: file_processor = Gestionnaire pour traiter diffÃ©rents types de fichiers
        Ligne 4: dimension = Taille des vecteurs (768 pour BGE-base-en-v1.5)
        Lignes 5-7: Configuration de l'algorithme HNSW pour FAISS
        Lignes 8-10: Chemins des fichiers de sauvegarde
        """
        self.ollama_api = ollama_api                                           # Ligne 1
        self.embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')    # Ligne 2
        self.file_processor = FileProcessor(chunk_size, chunk_overlap)         # Ligne 3
        self.dimension = 768  # Correct pour BAAI/bge-base-en-v1.5             # Ligne 4
        self.MCNoeud = 32                                                      # Ligne 5
        self.efSearch = 100                                                    # Ligne 6
        self.efConstruction = 80                                               # Ligne 7
        self.faiss_index_file = faiss_index_file                              # Ligne 8
        self.metadata_file = metadata_file                                     # Ligne 9
        self.hashes_file = hashes_file                                         # Ligne 10

        # Chargement ou initialisation des composants
        self.index = self.load_or_initialize_index()                          # Ligne 11
        self.metadata = self.load_or_initialize_metadata()                    # Ligne 12
        self.load_processed_hashes()                                          # Ligne 13
```

### 3.3 Explication DÃ©taillÃ©e : Initialisation

**Lignes 1-4 : Composants essentiels**
- `ollama_api` : Permet d'envoyer des requÃªtes au modÃ¨le de langage local
- `embedding_model` : Transforme le texte en vecteurs de 768 dimensions
- `file_processor` : GÃ¨re la lecture et le dÃ©coupage des fichiers
- `dimension = 768` : Chaque mot/phrase devient un vecteur de 768 nombres

**Lignes 5-7 : Configuration HNSW (Hierarchical Navigable Small World)**
- `MCNoeud = 32` : Nombre de connexions par nÅ“ud dans le graphe
- `efSearch = 100` : Nombre de candidats explorÃ©s lors de la recherche
- `efConstruction = 80` : ParamÃ¨tre de construction de l'index

**Lignes 8-13 : Gestion de la persistance**
- Les donnÃ©es sont sauvegardÃ©es sur disque pour Ã©viter de tout recalculer
- `index` : Structure FAISS contenant les vecteurs
- `metadata` : Informations sur chaque chunk (texte, fichier source, etc.)
- `hashes` : Ã‰vite de traiter deux fois le mÃªme fichier

### 3.4 Code Complet : Chargement et Initialisation des Index

ğŸ“ **Fichier : `rag_chatbot.py` (mÃ©thodes de la classe RAGChatbot)**

```python
def load_or_initialize_index(self):
    """
    Charge l'index FAISS depuis le disque ou crÃ©e un nouveau si inexistant.
    
    Ligne 1: VÃ©rification si le fichier d'index existe dÃ©jÃ 
    Ligne 2: Si existe, chargement de l'index sauvegardÃ©
    Ligne 4-6: Sinon, crÃ©ation d'un nouvel index HNSW avec produit scalaire
    Ligne 7-8: Configuration des paramÃ¨tres de recherche et construction
    """
    if os.path.exists(self.faiss_index_file):                                # Ligne 1
        return faiss.read_index(self.faiss_index_file)                       # Ligne 2
    else:
        index = faiss.IndexHNSWFlat(self.dimension, self.MCNoeud,            # Ligne 4
                                   faiss.METRIC_INNER_PRODUCT)               # Ligne 5
        index.hnsw.efSearch = self.efSearch                                  # Ligne 6
        index.hnsw.efConstruction = self.efConstruction                      # Ligne 7
        return index                                                         # Ligne 8

def load_or_initialize_metadata(self):
    """
    Charge les mÃ©tadonnÃ©es (informations sur chaque chunk) depuis le disque.
    
    Ligne 1: VÃ©rification si le fichier de mÃ©tadonnÃ©es existe
    Ligne 2-3: Si existe, chargement avec pickle (dÃ©sÃ©rialisation)
    Ligne 5: Sinon, crÃ©ation d'une liste vide pour stocker les futures mÃ©tadonnÃ©es
    """
    if os.path.exists(self.metadata_file):                                  # Ligne 1
        with open(self.metadata_file, 'rb') as f:                           # Ligne 2
            return pickle.load(f)                                           # Ligne 3
    else:
        return []                                                           # Ligne 5
```

### 3.5 Code Complet : Processus d'Ingestion des Documents

ğŸ“ **Fichier : `rag_chatbot.py` (mÃ©thode `ingestion_file` de la classe RAGChatbot)**

```python
def ingestion_file(self, base_filename, file_content, departement_id, filiere_id, 
                  module_id, activite_id, profile_id, user_id):
    """
    Traite un fichier complet : extraction, chunking, vectorisation, stockage.
    """
    try:
        # Ã‰tape 1: Traitement du fichier par le FileProcessor
        chunks, file_hash = self.file_processor.process_file(base_filename, file_content)  # Ligne 1
        
        # VÃ©rifications de base
        if chunks is None:                                                   # Ligne 2
            print(f"Traitement de {base_filename} annulÃ© (dÃ©jÃ  traitÃ©).")    # Ligne 3
            if file_hash:                                                    # Ligne 4
                raise ValueError(f"Le fichier {base_filename} a dÃ©jÃ  Ã©tÃ© traitÃ©.")  # Ligne 5
            else:
                raise ValueError(f"Le contenu du fichier {base_filename} n'a pas pu Ãªtre traitÃ©.")  # Ligne 7
        
        if not chunks:                                                       # Ligne 8
            print(f"Aucun chunk extrait de {base_filename}.")               # Ligne 9
            return                                                           # Ligne 10

        # Ã‰tape 2: GÃ©nÃ©ration des embeddings pour chaque chunk
        embeddings = []                                                      # Ligne 11
        for chunk_text_content in chunks:                                   # Ligne 12
            # Conversion du texte en vecteur numÃ©rique normalisÃ©
            embedding = self.embedding_model.encode([chunk_text_content],    # Ligne 13
                                                  normalize_embeddings=True)[0]  # Ligne 14
            embeddings.append(embedding)                                    # Ligne 15

        if not embeddings:                                                   # Ligne 16
            print(f"Aucun embedding gÃ©nÃ©rÃ© pour {base_filename}.")          # Ligne 17
            raise ValueError(f"Ã‰chec de la gÃ©nÃ©ration d'embeddings.")       # Ligne 18

        # Ã‰tape 3: Ajout des vecteurs Ã  l'index FAISS
        embeddings_np = np.array(embeddings, dtype='float32')               # Ligne 19
        start_index = self.index.ntotal                                     # Ligne 20
        self.index.add(embeddings_np)                                       # Ligne 21

        # Ã‰tape 4: Sauvegarde des mÃ©tadonnÃ©es pour chaque chunk
        for i, chunk_text_content in enumerate(chunks):                     # Ligne 22
            current_global_chunk_index = start_index + i                    # Ligne 23
            self.metadata.append({                                          # Ligne 24
                "file_hash": file_hash,                                     # Ligne 25
                "original_filename": base_filename,                         # Ligne 26
                "faiss_index": current_global_chunk_index,                  # Ligne 27
                "chunk_text": chunk_text_content,                          # Ligne 28
                "departement_id": departement_id,                          # Ligne 29
                "filiere_id": filiere_id,                                  # Ligne 30
                "module_id": module_id,                                    # Ligne 31
                "activite_id": activite_id,                                # Ligne 32
                "profile_id": profile_id,                                  # Ligne 33
                "user_id": user_id                                         # Ligne 34
            })
            
            # Sauvegarde dans la base de donnÃ©es SQLite
            FilterManager.insert_metadata_sqlite(                          # Ligne 35
                base_filename=base_filename, file_hash=file_hash,           # Ligne 36
                chunk_index=current_global_chunk_index,                     # Ligne 37
                chunk_text=chunk_text_content,                             # Ligne 38
                departement_id=departement_id, filiere_id=filiere_id,       # Ligne 39
                module_id=module_id, activite_id=activite_id,               # Ligne 40
                profile_id=profile_id, user_id=user_id                      # Ligne 41
            )
        
        # Sauvegarde de l'Ã©tat complet
        self.save_state()                                                   # Ligne 42
        print(f"Fichier {base_filename} indexÃ©. {len(chunks)} chunks ajoutÃ©s.")  # Ligne 43
        print(f"L'index Faiss contient maintenant {self.index.ntotal} vecteurs.")  # Ligne 44
        
    except Exception as e:
        print(f"Erreur lors de l'indexation de {base_filename} : {str(e)}")  # Ligne 45
        raise                                                               # Ligne 46
```

### 3.6 Explication DÃ©taillÃ©e : Ingestion

**Lignes 1-10 : Traitement et vÃ©rifications**
- `process_file()` extrait le texte et le dÃ©coupe en chunks
- `file_hash` permet de dÃ©tecter les doublons
- Si `chunks is None`, le fichier a dÃ©jÃ  Ã©tÃ© traitÃ© ou est vide

**Lignes 11-18 : Vectorisation**
- Chaque chunk de texte est transformÃ© en vecteur de 768 dimensions
- `normalize_embeddings=True` normalise les vecteurs (longueur = 1)
- Cette normalisation amÃ©liore la qualitÃ© de la recherche par similaritÃ©

**Lignes 19-21 : Ajout Ã  FAISS**
- Conversion en array NumPy avec type float32 (optimisÃ© pour FAISS)
- `start_index` = position actuelle dans l'index pour numÃ©roter les nouveaux vecteurs
- `add()` insÃ¨re tous les vecteurs d'un coup (plus efficace)

**Lignes 22-41 : MÃ©tadonnÃ©es et traÃ§abilitÃ©**
- Pour chaque chunk, on sauvegarde son texte et ses informations contextuelles
- `current_global_chunk_index` = identifiant unique dans FAISS
- Double sauvegarde : mÃ©moire (pour rapiditÃ©) + SQLite (pour persistance)

### 3.7 Les Ã‰tapes du Processus RAG (suite)

#### ğŸ“¥ **Ã‰tape 1 : Ingestion des Documents** âœ…

ComplÃ©tÃ©e ci-dessus avec le code dÃ©taillÃ©.

#### âœ‚ï¸ **Ã‰tape 2 : Chunking (DÃ©coupage en Segments)**

ğŸ“ **Fichier : `file_processor.py` (mÃ©thode `split_into_chunks` de la classe FileProcessor)**

```python
def split_into_chunks(self, text):
    """
    DÃ©coupe un texte long en segments plus petits avec chevauchement.
    
    Ligne 1: VÃ©rification que le texte n'est pas vide
    Ligne 2: Initialisation de la liste des chunks
    Ligne 3: Position de dÃ©but du dÃ©coupage
    Ligne 4: Longueur totale du texte Ã  traiter
    Ligne 5-10: Boucle de dÃ©coupage avec chevauchement
    """
    if not text: return []                                                   # Ligne 1
    chunks = []                                                              # Ligne 2
    start = 0                                                                # Ligne 3
    text_length = len(text)                                                  # Ligne 4
    while start < text_length:                                               # Ligne 5
        end = start + self.chunk_size                                        # Ligne 6
        chunk = text[start:end].strip()                                      # Ligne 7
        if chunk:                                                            # Ligne 8
            chunks.append(chunk)                                             # Ligne 9
        start += self.chunk_size - self.chunk_overlap                        # Ligne 10
    return chunks                                                            # Ligne 11
```

### 3.8 Explication DÃ©taillÃ©e : Chunking

**Ligne 6 : Calcul de la fin du chunk**
- `self.chunk_size = 384` caractÃ¨res par dÃ©faut
- Chaque segment fait donc maximum 384 caractÃ¨res

**Ligne 7 : Extraction et nettoyage**
- `text[start:end]` extrait la portion de texte
- `.strip()` supprime les espaces en dÃ©but/fin

**Ligne 10 : Chevauchement intelligent**
- `self.chunk_overlap = 96` caractÃ¨res
- Au lieu d'avancer de 384, on avance seulement de 384-96 = 288
- Les 96 derniers caractÃ¨res du chunk prÃ©cÃ©dent se retrouvent dans le suivant

**Pourquoi ce chevauchement ?**
1. **PrÃ©servation du contexte** : Ã‰vite de couper au milieu d'une phrase importante
2. **Meilleure recherche** : Une information Ã  cheval sur deux chunks reste accessible
3. **Robustesse** : Compense les imperfections du dÃ©coupage automatique

**Exemple concret :**
```
Document : "L'intelligence artificielle est une technologie rÃ©volutionnaire. Elle permet d'automatiser de nombreuses tÃ¢ches complexes. Les applications sont multiples..."

Chunk 1 (0-384) : "L'intelligence artificielle est une technologie rÃ©volutionnaire. Elle permet d'automatiser de nombreuses tÃ¢ches complexes. Les applications..."

Chunk 2 (288-672) : "...tÃ¢ches complexes. Les applications sont multiples dans de nombreux domaines comme la mÃ©decine, la finance..."
                     â†‘ Chevauchement de 96 caractÃ¨res â†‘
```

#### ğŸ”¢ **Ã‰tape 3 : Vectorisation (Embeddings)**

ğŸ“ **Fichier : `rag_chatbot.py` (dans la mÃ©thode `ingestion_file`)**

```python
# Dans la mÃ©thode ingestion_file, lignes 11-18
embeddings = []                                                      # Ligne 11
for chunk_text_content in chunks:                                   # Ligne 12
    # Conversion du texte en vecteur numÃ©rique normalisÃ©
    embedding = self.embedding_model.encode([chunk_text_content],    # Ligne 13
                                          normalize_embeddings=True)[0]  # Ligne 14
    embeddings.append(embedding)                                    # Ligne 15
```

### 3.9 Explication DÃ©taillÃ©e : Vectorisation (Embeddings)

**Ligne 11 : Initialisation de la liste**
- `embeddings = []` crÃ©e une liste vide pour stocker tous les vecteurs

**Ligne 12 : Boucle sur chaque chunk**
- Pour chaque segment de texte, nous allons crÃ©er un vecteur numÃ©rique

**Lignes 13-14 : Transformation en vecteur**
- `self.embedding_model` utilise le modÃ¨le `BAAI/bge-base-en-v1.5`
- `encode([chunk_text_content])` convertit le texte en vecteur
- `normalize_embeddings=True` normalise le vecteur (longueur = 1)
- `[0]` rÃ©cupÃ¨re le premier (et unique) vecteur du rÃ©sultat

**Ligne 15 : Stockage**
- Ajoute le vecteur Ã  la liste pour traitement ultÃ©rieur

**Qu'est-ce qu'un embedding concrÃ¨tement ?**
```
Texte : "L'intelligence artificielle"
Vecteur : [0.23, -0.15, 0.67, 0.12, 0.45, ..., 0.34]  (768 nombres)

Texte similaire : "L'IA et machine learning"  
Vecteur : [0.21, -0.18, 0.69, 0.09, 0.48, ..., 0.29]  (proche du prÃ©cÃ©dent)
```

### 3.10 Code Complet : Recherche avec MMR

ğŸ“ **Fichier : `rag_chatbot.py` (mÃ©thode `find_relevant_context` de la classe RAGChatbot)**

```python
def find_relevant_context(self, user_query, departement_id=None, filiere_id=None,
                          top_k=3, similarity_threshold=0.65, use_mmr=False, mmr_lambda=0.5):
    """
    Recherche les chunks les plus pertinents pour une requÃªte utilisateur.
    Utilise le filtrage acadÃ©mique et optionnellement MMR pour la diversitÃ©.
    """
    try:
        # Ã‰tape 1: Conversion de la question en vecteur
        query_embedding = self.embedding_model.encode([user_query],          # Ligne 1
                                                    normalize_embeddings=True)[0]  # Ligne 2
        normalized_query = query_embedding.reshape(1, -1)                   # Ligne 3
    except Exception as e:
        print(f"Erreur lors de la gÃ©nÃ©ration de l'embedding: {e}")          # Ligne 4
        return None                                                         # Ligne 5

    # Ã‰tape 2: VÃ©rification que l'index FAISS contient des donnÃ©es
    if not hasattr(self.index, 'ntotal') or self.index.ntotal == 0:        # Ligne 6
        print("Aucun contexte indexÃ© dans Faiss.")                         # Ligne 7
        return None                                                         # Ligne 8

    try:
        # Ã‰tape 3: RÃ©cupÃ©ration des chunks autorisÃ©s selon les filtres acadÃ©miques
        allowed_faiss_ids = FilterManager.get_allowed_indices(              # Ligne 9
            departement_id, filiere_id                                      # Ligne 10
        )
    except Exception as e:
        print(f"Erreur lors de la rÃ©cupÃ©ration des IDs autorisÃ©s: {e}")    # Ligne 11
        return None                                                         # Ligne 12

    if not allowed_faiss_ids:                                               # Ligne 13
        print("Aucun chunk autorisÃ© trouvÃ©.")                              # Ligne 14
        return None                                                         # Ligne 15

    # Ã‰tape 4: PrÃ©paration des IDs valides pour la reconstruction
    allowed_faiss_ids_array = np.array(list(allowed_faiss_ids), dtype='int64')  # Ligne 16
    valid_ids_for_reconstruction = np.array(                               # Ligne 17
        [fid for fid in allowed_faiss_ids_array if 0 <= fid < self.index.ntotal],  # Ligne 18
        dtype='int64'                                                       # Ligne 19
    )

    if len(valid_ids_for_reconstruction) == 0:                              # Ligne 20
        print("Aucun ID Faiss valide dans l'index principal.")             # Ligne 21
        return None                                                         # Ligne 22

    # Ã‰tape 5: Reconstruction des vecteurs depuis FAISS
    sub_vectors_list = []                                                   # Ligne 23
    try:
        reconstruction_source = self.index.storage                         # Ligne 24
        for vector_id in valid_ids_for_reconstruction:                      # Ligne 25
            reconstructed_vec = reconstruction_source.reconstruct(int(vector_id))  # Ligne 26
            sub_vectors_list.append(reconstructed_vec)                      # Ligne 27
        
        sub_vectors = np.array(sub_vectors_list, dtype='float32')           # Ligne 28
    except Exception as e:
        print(f"Erreur lors de la reconstruction des vecteurs: {e}")       # Ligne 29
        return None                                                         # Ligne 30

    # Ã‰tape 6: CrÃ©ation d'un sous-index pour la recherche
    try:
        sub_index = faiss.IndexFlatIP(self.dimension)                       # Ligne 31
        sub_index.add(sub_vectors)                                          # Ligne 32
    except Exception as e:
        print(f"Erreur lors de la crÃ©ation du sous-index: {e}")            # Ligne 33
        return None                                                         # Ligne 34

    # Ã‰tape 7: Recherche par similaritÃ©
    try:
        k_search = min(top_k * 5 if use_mmr else top_k, sub_index.ntotal)   # Ligne 35
        distances, local_indices_in_sub = sub_index.search(normalized_query, k_search)  # Ligne 36
    except Exception as e:
        print(f"Erreur lors de la recherche: {e}")                         # Ligne 37
        return None                                                         # Ligne 38

    # Ã‰tape 8: Filtrage par seuil de similaritÃ© et application de MMR si demandÃ©
    candidate_embeddings = [sub_vectors[i] for i in local_indices_in_sub[0]  # Ligne 39
                            if i >= 0 and distances[0][list(local_indices_in_sub[0]).index(i)] >= similarity_threshold]  # Ligne 40
    candidate_indices = [i for i in local_indices_in_sub[0]                 # Ligne 41
                        if i >= 0 and distances[0][list(local_indices_in_sub[0]).index(i)] >= similarity_threshold]  # Ligne 42

    if use_mmr and candidate_embeddings:                                    # Ligne 43
        mmr_indices = self.mmr(normalized_query.flatten(), candidate_embeddings,  # Ligne 44
                              k=top_k, lambda_param=mmr_lambda)            # Ligne 45
        selected_indices = [candidate_indices[i] for i in mmr_indices]     # Ligne 46
    else:
        selected_indices = candidate_indices[:top_k]                        # Ligne 47

    # Ã‰tape 9: RÃ©cupÃ©ration des textes correspondants
    relevant_chunks_texts = []                                              # Ligne 48
    for i in selected_indices:                                              # Ligne 49
        global_faiss_id = valid_ids_for_reconstruction[i]                   # Ligne 50
        chunk_data = None                                                   # Ligne 51
        for meta_item in self.metadata:                                     # Ligne 52
            if meta_item.get("faiss_index") == global_faiss_id:             # Ligne 53
                chunk_data = meta_item                                      # Ligne 54
                break                                                       # Ligne 55
        if chunk_data and 'chunk_text' in chunk_data:                       # Ligne 56
            relevant_chunks_texts.append(chunk_data['chunk_text'])          # Ligne 57

    if not relevant_chunks_texts:                                           # Ligne 58
        print("Aucun chunk pertinent trouvÃ©.")                             # Ligne 59
        return None                                                         # Ligne 60
    
    return relevant_chunks_texts                                            # Ligne 61
```

### 3.11 Explication DÃ©taillÃ©e : Recherche avec Filtrage AcadÃ©mique

**Lignes 1-5 : Conversion de la question en vecteur**
- `user_query` (ex: "Qu'est-ce que l'IA?") â†’ vecteur de 768 dimensions
- `normalize_embeddings=True` normalise le vecteur pour une meilleure recherche
- `reshape(1, -1)` transforme en format attendu par FAISS (matrice 1Ã—768)

**Lignes 6-8 : VÃ©rification de l'index**
- `self.index.ntotal` = nombre total de vecteurs stockÃ©s
- Si 0, aucun document n'a Ã©tÃ© indexÃ© â†’ impossible de rÃ©pondre

**Lignes 9-15 : Filtrage acadÃ©mique**
- `FilterManager.get_allowed_indices()` rÃ©cupÃ¨re les IDs autorisÃ©s
- Selon le dÃ©partement (ex: Informatique) et filiÃ¨re (ex: GÃ©nie Logiciel)
- Seuls les chunks pertinents pour l'Ã©tudiant sont considÃ©rÃ©s

**Lignes 16-22 : Validation des IDs**
- Conversion en array NumPy pour optimisation
- Filtrage des IDs valides (0 â‰¤ ID < nombre total de vecteurs)
- Protection contre les erreurs d'index

**Lignes 23-30 : Reconstruction des vecteurs**
- `self.index.storage` = zone de stockage des vecteurs dans FAISS
- `reconstruct(vector_id)` rÃ©cupÃ¨re le vecteur original depuis son ID
- NÃ©cessaire car on ne peut pas chercher directement dans un sous-ensemble

**Lignes 31-34 : CrÃ©ation du sous-index**
- `IndexFlatIP` = index plat avec produit scalaire (Inner Product)
- Contient uniquement les vecteurs autorisÃ©s pour l'Ã©tudiant
- Plus rapide que filtrer aprÃ¨s la recherche

**Lignes 35-38 : Recherche par similaritÃ©**
- `k_search` = nombre de candidats (Ã—5 si MMR activÃ©)
- `search()` trouve les vecteurs les plus similaires Ã  la question
- Retourne distances (scores de similaritÃ©) et indices

**Lignes 39-47 : Application de MMR (optionnelle)**
- Filtrage par seuil (`similarity_threshold = 0.65`)
- Si MMR activÃ© : diversification des rÃ©sultats
- Sinon : prise des `top_k` plus pertinents

**Lignes 48-61 : RÃ©cupÃ©ration des textes**
- Pour chaque indice sÃ©lectionnÃ© â†’ rÃ©cupÃ©ration du texte original
- Recherche dans `self.metadata` pour retrouver le contenu textuel
- Retour de la liste des chunks pertinents

### 3.12 Code Complet : Algorithme MMR (Maximal Marginal Relevance)

ğŸ“ **Fichier : `rag_chatbot.py` (mÃ©thode `mmr` de la classe RAGChatbot)**

```python
def mmr(self, query_embedding, candidate_embeddings, k=3, lambda_param=0.5):
    """
    Maximal Marginal Relevance pour diversifier les chunks sÃ©lectionnÃ©s.
    
    query_embedding: vecteur de la question (shape: dim,)
    candidate_embeddings: liste des vecteurs candidats (shape: n, dim)
    k: nombre de chunks Ã  retourner
    lambda_param: balance pertinence/diversitÃ© (0.0=diversitÃ© max, 1.0=pertinence max)
    """
    selected = []                                                           # Ligne 1
    selected_indices = []                                                   # Ligne 2
    candidate_indices = list(range(len(candidate_embeddings)))             # Ligne 3
    query_embedding = query_embedding.reshape(1, -1)                       # Ligne 4
    candidate_embeddings = np.array(candidate_embeddings)                  # Ligne 5
    
    # Calcul de la similaritÃ© entre la question et chaque candidat
    sim_to_query = np.dot(candidate_embeddings, query_embedding.T).flatten()  # Ligne 6
    
    # SÃ©lection itÃ©rative des meilleurs candidats
    for _ in range(min(k, len(candidate_embeddings))):                      # Ligne 7
        if not selected:                                                    # Ligne 8
            # Premier candidat : le plus pertinent
            idx = int(np.argmax(sim_to_query))                              # Ligne 9
            selected.append(candidate_embeddings[idx])                      # Ligne 10
            selected_indices.append(idx)                                    # Ligne 11
            candidate_indices.remove(idx)                                   # Ligne 12
        else:
            # Candidats suivants : Ã©quilibre pertinence/diversitÃ©
            max_score = -np.inf                                             # Ligne 13
            max_idx = -1                                                    # Ligne 14
            
            for idx in candidate_indices:                                   # Ligne 15
                # Calcul de la pertinence par rapport Ã  la question
                relevance = sim_to_query[idx]                               # Ligne 16
                
                # Calcul de la diversitÃ© (max similaritÃ© avec les sÃ©lectionnÃ©s)
                diversity = max([np.dot(candidate_embeddings[idx], s)       # Ligne 17
                               for s in selected])                          # Ligne 18
                
                # Score MMR = Ã©quilibre pertinence - diversitÃ©
                mmr_score = lambda_param * relevance - (1 - lambda_param) * diversity  # Ligne 19
                
                # MÃ©moriser le meilleur candidat
                if mmr_score > max_score:                                   # Ligne 20
                    max_score = mmr_score                                   # Ligne 21
                    max_idx = idx                                           # Ligne 22
            
            # Ajouter le meilleur candidat Ã  la sÃ©lection
            selected.append(candidate_embeddings[max_idx])                  # Ligne 23
            selected_indices.append(max_idx)                               # Ligne 24
            candidate_indices.remove(max_idx)                              # Ligne 25
    
    return selected_indices                                                 # Ligne 26
```

### 3.13 Explication DÃ©taillÃ©e : Algorithme MMR

**Lignes 1-6 : Initialisation**
- `selected` : liste des vecteurs dÃ©jÃ  choisis
- `selected_indices` : indices des vecteurs choisis 
- `candidate_indices` : liste des candidats restants (0, 1, 2, ...)
- `sim_to_query` : similaritÃ© de chaque candidat avec la question

**Ligne 6 : Calcul des similaritÃ©s**
- `np.dot()` calcule le produit scalaire (mesure de similaritÃ©)
- Plus le score est Ã©levÃ©, plus le candidat est pertinent

**Lignes 8-12 : Premier candidat**
- Pour le premier choix, on prend simplement le plus pertinent
- `np.argmax()` trouve l'indice du maximum
- On l'ajoute aux sÃ©lectionnÃ©s et le retire des candidats

**Lignes 13-25 : Candidats suivants**
- Pour chaque candidat restant, calcul du score MMR
- `relevance` = Ã  quel point il rÃ©pond Ã  la question
- `diversity` = Ã  quel point il est diffÃ©rent des dÃ©jÃ  sÃ©lectionnÃ©s
- `mmr_score` = Ã©quilibre entre les deux selon `lambda_param`

**La formule MMR :**
```
Score = Î» Ã— Pertinence - (1-Î») Ã— DiversitÃ©

Î» = 1.0 : Seulement la pertinence compte
Î» = 0.0 : Seulement la diversitÃ© compte  
Î» = 0.5 : Ã‰quilibre 50/50
```

**Exemple pratique :**

Question : "Qu'est-ce que le machine learning ?"

Candidats trouvÃ©s :
1. "Le machine learning est une branche de l'IA..." (Pertinence: 0.9)
2. "Le ML utilise des algorithmes d'apprentissage..." (Pertinence: 0.85, SimilaritÃ© avec 1: 0.8)
3. "Les applications ML incluent la vision par ordinateur..." (Pertinence: 0.7, SimilaritÃ© avec 1: 0.3)

Avec Î» = 0.5 :
- Candidat 1 sÃ©lectionnÃ© en premier (plus pertinent)
- Candidat 2 : Score = 0.5Ã—0.85 - 0.5Ã—0.8 = 0.025
- Candidat 3 : Score = 0.5Ã—0.7 - 0.5Ã—0.3 = 0.2

â†’ Candidat 3 sÃ©lectionnÃ© (plus diversifiÃ©)

### 3.14 Code Complet : GÃ©nÃ©ration de RÃ©ponse

ğŸ“ **Fichier : `rag_chatbot.py` (mÃ©thode `generate_response` de la classe RAGChatbot)**

```python
def generate_response(self, user_query, user_id, profile_id, departement_id, filiere_id, use_mmr=False):
    """
    GÃ©nÃ¨re une rÃ©ponse complÃ¨te Ã  partir de la question utilisateur.
    IntÃ¨gre recherche RAG, construction de prompt et gÃ©nÃ©ration LLM.
    """
    INVITE_PROFILE_ID = 5                                                   # Ligne 1

    # Gestion du profil invitÃ© avec restrictions
    is_invite = (profile_id == INVITE_PROFILE_ID)                          # Ligne 2
    if is_invite:                                                           # Ligne 3
        departement_id = 1  # DÃ©partement ScolaritÃ© uniquement             # Ligne 4
        filiere_id = 3                                                      # Ligne 5

    # Ã‰tape 1: Recherche des chunks pertinents avec RAG
    context_chunks = self.find_relevant_context(                           # Ligne 6
        user_query, departement_id, filiere_id,                            # Ligne 7
        top_k=3, similarity_threshold=0.65, use_mmr=use_mmr                # Ligne 8
    )

    # Ã‰tape 2: Construction du prompt selon le type de demande
    if context_chunks:                                                      # Ligne 9
        context_text = "\n".join(context_chunks)                           # Ligne 10
        
        # DÃ©tection du type de demande
        if PromptBuilder.is_qcm_request(user_query):                        # Ligne 11
            prompt_text = PromptBuilder.build_qcm_prompt(context_text, user_query)  # Ligne 12
        elif "rÃ©sumÃ©" in user_query.lower() or "synthÃ¨se" in user_query.lower():   # Ligne 13
            prompt_text = PromptBuilder.build_summary_prompt(context_text, user_query)  # Ligne 14
        else:
            prompt_text = PromptBuilder.build_standard_prompt(context_text, user_query)  # Ligne 15

        # Gestion spÃ©ciale pour les invitÃ©s
        if is_invite:                                                       # Ligne 16
            prompt_text = (                                                 # Ligne 17
                "âš ï¸ **Mode invitÃ© activÃ© :**\n"                            # Ligne 18
                "Tu dois rÃ©pondre **uniquement** Ã  partir des informations fournies par le dÃ©partement ScolaritÃ©.\n"  # Ligne 19
                "Si la question ne concerne pas le dÃ©partement ScolaritÃ©, ou si tu n'as pas l'information dans le contexte, rÃ©ponds exactement :\n"  # Ligne 20
                "\"Je ne peux rÃ©pondre qu'aux questions relatives au dÃ©partement ScolaritÃ©.\"\n"  # Ligne 21
                "N'ajoute rien d'autre, n'explique pas, ne propose pas de ressources, ne donne aucune information supplÃ©mentaire.\n"  # Ligne 22
                "N'utilise **aucune autre information**, mÃªme si elle est prÃ©sente dans le contexte.\n"  # Ligne 23
                "N'intÃ¨gre pas de ressources supplÃ©mentaires extÃ©rieures au dÃ©partement ScolaritÃ©.\n\n"  # Ligne 24
                "**Gestion des Ressources :**\n"                           # Ligne 25
                "La section Â« ğŸ“š Pour Aller Plus Loin Â» doit apparaÃ®tre **uniquement si des ressources pertinentes sont disponibles** dans le contexte du dÃ©partement ScolaritÃ©.\n"  # Ligne 26
                "Si aucune ressource n'est pertinente, **n'inclus pas cette section**.\n\n"  # Ligne 27
                "**Important :**\n"                                         # Ligne 28
                "Ne rÃ©ponds Ã  aucune question qui ne concerne pas le dÃ©partement ScolaritÃ©. Ignore toute demande hors de ce pÃ©rimÃ¨tre.\n"  # Ligne 29
                "Exemple :\n"                                               # Ligne 30
                "Q : Qu'est-ce que PySpark ?\n"                            # Ligne 31
                "R : Je ne peux rÃ©pondre qu'aux questions relatives au dÃ©partement ScolaritÃ©.\n"  # Ligne 32
            ) + prompt_text                                                 # Ligne 33

    else:
        # Aucun contexte trouvÃ©
        prompt_text = (                                                     # Ligne 34
            f"Question : {user_query}\n"                                    # Ligne 35
            "RÃ©ponds que tu n'as pas d'informations pertinentes pour cette question."  # Ligne 36
        )

    # Ã‰tape 3: GÃ©nÃ©ration de la rÃ©ponse avec le LLM
    llm_raw_response = self.ollama_api.chat_with_ollama(prompt_text)        # Ligne 37
    cleaned_llm_response = self.clean_llm_response(llm_raw_response)        # Ligne 38

    # Ã‰tape 4: Sauvegarde dans l'historique
    chat_id = FilterManager.save_chat_history(                             # Ligne 39
        user_id=user_id,                                                    # Ligne 40
        question=user_query,                                                # Ligne 41
        answer=cleaned_llm_response,                                        # Ligne 42
        departement_id=departement_id,                                      # Ligne 43
        filiere_id=filiere_id,                                              # Ligne 44
        profile_id=profile_id,                                              # Ligne 45
        db_path=self.SQLITE_DB_PATH                                         # Ligne 46
    )

    return cleaned_llm_response, chat_id                                    # Ligne 47
```

### 3.15 Code Complet : Nettoyage des RÃ©ponses LLM

ğŸ“ **Fichier : `rag_chatbot.py` (mÃ©thode `clean_llm_response` de la classe RAGChatbot)**

```python
def clean_llm_response(self, response: str) -> str:
    """
    Nettoie la rÃ©ponse brute du LLM en supprimant les balises indÃ©sirables.
    
    Ligne 1: Extraction aprÃ¨s la derniÃ¨re balise de fermeture
    Lignes 2-3: Suppression des balises <think> (mode rÃ©flexion du LLM)
    Ligne 4: Suppression des espaces en dÃ©but/fin
    """
    # Extraire le contenu aprÃ¨s la derniÃ¨re balise de fermeture
    if '</' in response:                                                    # Ligne 1
        response = response.split('</')[-1]                                 # Ligne 2

    # Nettoyage des balises de rÃ©flexion
    cleaned = re.sub(r"<?think>", "", response, flags=re.DOTALL | re.IGNORECASE)  # Ligne 3
    cleaned = re.sub(r"</?think>", "", cleaned, flags=re.IGNORECASE)       # Ligne 4
    
    return cleaned.strip()                                                  # Ligne 5
```

### 3.16 Explication DÃ©taillÃ©e : GÃ©nÃ©ration de RÃ©ponse

**Lignes 1-5 : Gestion des profils utilisateur**
- `INVITE_PROFILE_ID = 5` : identifiant du profil invitÃ©
- Les invitÃ©s sont limitÃ©s au dÃ©partement ScolaritÃ© uniquement
- EmpÃªche l'accÃ¨s aux contenus techniques ou spÃ©cialisÃ©s

**Lignes 6-8 : Recherche RAG**
- Appel de `find_relevant_context()` avec tous les paramÃ¨tres
- `top_k=3` : maximum 3 chunks retournÃ©s
- `similarity_threshold=0.65` : seuil de pertinence
- `use_mmr` : activation optionnelle de la diversification

**Lignes 9-15 : Construction du prompt adaptatif**
- `context_text` : concatÃ©nation des chunks trouvÃ©s
- DÃ©tection automatique du type de demande :
  - QCM : mots-clÃ©s "qcm", "quiz", "choix multiple"
  - RÃ©sumÃ© : mots "rÃ©sumÃ©", "synthÃ¨se"
  - Standard : autres demandes

**Lignes 16-33 : Restrictions pour invitÃ©s**
- Prompt spÃ©cialisÃ© avec consignes strictes
- Limitation au dÃ©partement ScolaritÃ© uniquement
- RÃ©ponse standardisÃ©e si hors pÃ©rimÃ¨tre
- ContrÃ´le de l'affichage des ressources

**Lignes 34-36 : Gestion absence de contexte**
- Si aucun chunk pertinent trouvÃ©
- RÃ©ponse polie indiquant l'absence d'information

**Lignes 37-38 : GÃ©nÃ©ration et nettoyage**
- `chat_with_ollama()` : appel au modÃ¨le de langage
- `clean_llm_response()` : suppression des artÃ©facts

**Lignes 39-47 : TraÃ§abilitÃ©**
- Sauvegarde de la conversation en base
- Retour de la rÃ©ponse et de l'ID de la conversation

### 3.17 Code Complet : Traitement des Fichiers

ğŸ“ **Fichier : `file_processor.py` (classe FileProcessor)**

```python
import os
import hashlib
import json
import PyPDF2
from docx import Document
import io
from pptx import Presentation
from Utilitaire.EDA_Cleaner import TextPipeline, TextCleaner

class FileProcessor:
    def __init__(self, chunk_size=384, chunk_overlap=96):
        """
        Initialise le processeur de fichiers.
        
        chunk_size: taille maximum d'un segment en caractÃ¨res
        chunk_overlap: chevauchement entre segments en caractÃ¨res
        processed_hashes: ensemble des fichiers dÃ©jÃ  traitÃ©s
        """
        self.chunk_size = chunk_size                                        # Ligne 1
        self.chunk_overlap = chunk_overlap                                  # Ligne 2
        self.processed_hashes = set()                                       # Ligne 3

    def calculate_hash(self, content_string):
        """
        Calcule un hash SHA-256 du contenu pour dÃ©tecter les doublons.
        
        Ligne 1: Encodage du texte en UTF-8
        Ligne 2: Calcul du hash et conversion en hexadÃ©cimal
        """
        return hashlib.sha256(content_string.encode('utf-8')).hexdigest()   # Ligne 1

    def read_content_from_bytes(self, base_filename, file_content_bytes):
        """
        Convertit le contenu d'un fichier (bytes) en string selon son extension.
        
        Ligne 1: Extraction de l'extension du fichier
        Lignes 3-8: Traitement des fichiers texte
        Lignes 9-15: Traitement des PDF
        Lignes 16-20: Traitement des DOCX
        Lignes 21-24: Traitement des JSON
        Lignes 25-31: Traitement des PPTX
        """
        file_extension = os.path.splitext(base_filename)[1].lower()         # Ligne 1
        content_str = ""                                                    # Ligne 2

        try:
            if file_extension == '.txt':                                    # Ligne 3
                content_str = file_content_bytes.decode('utf-8')            # Ligne 4
                
            elif file_extension == '.pdf':                                  # Ligne 5
                pdf_stream = io.BytesIO(file_content_bytes)                 # Ligne 6
                pdf_reader = PyPDF2.PdfReader(pdf_stream)                   # Ligne 7
                for page in pdf_reader.pages:                               # Ligne 8
                    page_text = page.extract_text()                        # Ligne 9
                    if page_text:                                           # Ligne 10
                        content_str += page_text + "\n"                    # Ligne 11
                        
            elif file_extension == '.docx':                                 # Ligne 12
                docx_stream = io.BytesIO(file_content_bytes)                # Ligne 13
                doc = Document(docx_stream)                                 # Ligne 14
                for para in doc.paragraphs:                                 # Ligne 15
                    content_str += para.text + "\n"                        # Ligne 16
                    
            elif file_extension == '.json':                                 # Ligne 17
                json_string = file_content_bytes.decode('utf-8')            # Ligne 18
                data = json.loads(json_string)                              # Ligne 19
                content_str = json.dumps(data, ensure_ascii=False)          # Ligne 20
                
            elif file_extension == '.pptx':                                 # Ligne 21
                pptx_stream = io.BytesIO(file_content_bytes)                # Ligne 22
                prs = Presentation(pptx_stream)                             # Ligne 23
                for slide in prs.slides:                                    # Ligne 24
                    for shape in slide.shapes:                              # Ligne 25
                        if hasattr(shape, "text"):                          # Ligne 26
                            content_str += shape.text + "\n"               # Ligne 27
            else:
                raise ValueError(f"Type de fichier non supportÃ© : {file_extension}")  # Ligne 28

            return content_str.strip()                                      # Ligne 29
            
        except Exception as e:
            raise Exception(f"Erreur lecture {base_filename} ({file_extension}) : {str(e)}")  # Ligne 30

    def process_file(self, base_filename, file_content_bytes):
        """
        Traite complÃ¨tement un fichier : lecture, hash, nettoyage, chunking.
        
        Retourne (chunks, file_hash) ou (None, file_hash) si dÃ©jÃ  traitÃ©
        """
        try:
            # Ã‰tape 1: Extraction du contenu textuel
            content_string = self.read_content_from_bytes(base_filename, file_content_bytes)  # Ligne 1

            if not content_string:                                          # Ligne 2
                print(f"Aucun contenu textuel extrait de {base_filename}")  # Ligne 3
                return None, None                                           # Ligne 4

            # Ã‰tape 2: Calcul du hash pour dÃ©tecter les doublons
            file_hash = self.calculate_hash(content_string)                 # Ligne 5

            if file_hash in self.processed_hashes:                          # Ligne 6
                print(f"Fichier {base_filename} dÃ©jÃ  traitÃ© (hash: {file_hash})")  # Ligne 7
                return None, file_hash                                      # Ligne 8

            # Ã‰tape 3: Nettoyage du texte avec pipeline EDA
            cleaner = TextCleaner()                                         # Ligne 9
            pipeline = TextPipeline(cleaner)                                # Ligne 10
            cleaned_content = pipeline.process(content_string)              # Ligne 11

            if not cleaned_content:                                         # Ligne 12
                print(f"Contenu vide aprÃ¨s nettoyage pour {base_filename}") # Ligne 13
                return None, file_hash                                      # Ligne 14

            # Ã‰tape 4: DÃ©coupage en chunks
            chunks = self.split_into_chunks(cleaned_content)                # Ligne 15

            if not chunks:                                                  # Ligne 16
                print(f"Aucun chunk gÃ©nÃ©rÃ© pour {base_filename}")          # Ligne 17
                return None, file_hash                                      # Ligne 18

            # Si tout s'est bien passÃ©
            self.processed_hashes.add(file_hash)                            # Ligne 19
            return chunks, file_hash                                        # Ligne 20

        except Exception as e:
            print(f"Erreur dans FileProcessor pour {base_filename}: {str(e)}")  # Ligne 21
            raise                                                           # Ligne 22
```

---

## 4. Analyse de Sentiment {#sentiment}

### 4.1 Objectif de l'Analyse de Sentiment

L'analyse de sentiment permet de :
- **Comprendre** la satisfaction des Ã©tudiants
- **Identifier** les points d'amÃ©lioration
- **Adapter** le systÃ¨me selon les retours

### 4.2 Code Complet : Classe SimpleSentimentAnalyzer

ğŸ“ **Fichier : `sentiment_analyzer.py` (classe principale)**

```python
import pandas as pd
from ollama_api import OllamaAPI
from sqlalchemy import create_engine, text
import json
import os
import time
import logging
from dotenv import load_dotenv
from textblob import TextBlob
from langchain_groq import ChatGroq

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
DATABASE_URL = "sqlite:///./bdd/chatbot_metadata.db"

SQL_QUERY = """
SELECT id as ID, DÃ©partement, FiliÃ¨re, Profile, "User", FeedBack, "timestamp"
FROM "V_FeedBack"
WHERE polarity IS NULL
"""

class SimpleSentimentAnalyzer:
    def __init__(self):
        """
        Initialise l'analyseur de sentiment avec support multi-mÃ©thodes.
        
        Ligne 1: Initialisation de la variable LLM
        Lignes 2-14: Configuration du LLM Groq si disponible
        """
        self.llm = None                                                         # Ligne 1
        if GROQ_API_KEY:                                                        # Ligne 2
            try:
                self.llm = ChatGroq(                                            # Ligne 3
                    model_name="llama3-8b-8192",                               # Ligne 4
                    temperature=0,                                              # Ligne 5
                    api_key=GROQ_API_KEY,                                       # Ligne 6
                    max_retries=2,                                              # Ligne 7
                    request_timeout=30,                                         # Ligne 8
                    max_tokens=200                                              # Ligne 9
                )
                logger.info("LLM Groq initialisÃ© avec succÃ¨s")                 # Ligne 10
            except Exception as e:
                logger.warning(f"Impossible d'initialiser Groq: {e}")          # Ligne 11
                self.llm = None                                                 # Ligne 12

    def analyze_sentiment_rule_based(self, feedback_text):
        """
        Analyse de sentiment basÃ©e sur des rÃ¨gles linguistiques.
        MÃ©thode principale et la plus fiable.
        """
        feedback_lower = feedback_text.lower()                                 # Ligne 1
        
        # DÃ©finition des mots-clÃ©s pour dÃ©tecter le contenu pÃ©dagogique
        content_keywords = [                                                    # Ligne 2
            'contenu', 'cours', 'leÃ§on', 'rÃ©sumÃ©', 'explication', 'pÃ©dagogique',
            'matiÃ¨re', 'sujet', 'chapitre', 'module', 'formation', 'apprentissage',
            'rÃ©ponse', 'solution', 'information', 'connaissance', 'enseignement'
        ]
        
        # Mots-clÃ©s positifs
        positive_keywords = [                                                   # Ligne 3
            'gÃ©nial', 'super', 'magnifique', 'excellent', 'parfait', 'bien', 'bon',
            'merci', 'formidable', 'fantastique', 'top', 'bravo', 'impressionnant',
            'efficace', 'utile', 'clair', 'prÃ©cis', 'complet', 'satisfait', 
            'agrÃ©able', 'intÃ©ressant', 'captivant', 'engageant', 'instructif', 'pratique',
            'facile Ã  comprendre', 'bien structurÃ©', 'bien expliquÃ©',
            'trÃ¨s utile', 'trÃ¨s clair', 'trÃ¨s complet', 'trÃ¨s intÃ©ressant', 'trÃ¨s engageant'
        ]
        
        # Mots-clÃ©s nÃ©gatifs
        negative_keywords = [                                                   # Ligne 4
            'mauvais', 'nul', 'horrible', 'dÃ©cevant', 'problÃ¨me', 'erreur',
            'difficile', 'compliquÃ©', 'confus', 'incomprÃ©hensible', 'long',
            'court', 'bref', 'insuffisant', 'pas convaincu',
            'peu clair', 'peu prÃ©cis', 'peu complet', 'peu intÃ©ressant',
            'peu engageant', 'peu utile', 'peu pratique', 'pas utile',
            'pas pratique', 'pas efficace', 'pas satisfaisant', 'pas agrÃ©able',
        ]
        
        # Mots-clÃ©s neutres (critiques constructives)
        neutral_keywords = [                                                    # Ligne 5
            'correcte', 'acceptable', 'moyen', 'peut mieux faire', 'amÃ©liorer',
            'manque de', 'besoin de', 'pourrait Ãªtre mieux', 'manque de clartÃ©',
            'manque de dÃ©tails', "manque d'exemples", 'manque de structure'
        ]
        
        # VÃ©rification si le contenu pÃ©dagogique est mentionnÃ©
        has_content = any(keyword in feedback_lower for keyword in content_keywords)  # Ligne 6
        if not has_content:                                                     # Ligne 7
            return {"aspect": "Contenu pÃ©dagogique", "polarity": "non mentionnÃ©"}  # Ligne 8
        
        # Comptage des occurrences
        positive_count = sum(1 for keyword in positive_keywords                 # Ligne 9
                           if keyword in feedback_lower)                        # Ligne 10
        negative_count = sum(1 for keyword in negative_keywords                 # Ligne 11
                           if keyword in feedback_lower)                        # Ligne 12
        neutral_count = sum(1 for keyword in neutral_keywords                   # Ligne 13
                          if keyword in feedback_lower)                         # Ligne 14
        
        # Gestion des cas spÃ©ciaux
        if 'trop long' in feedback_lower or 'trop brÃ¨ve' in feedback_lower:     # Ligne 15
            negative_count += 1                                                 # Ligne 16
        
        if 'prÃ©fÃ¨re' in feedback_lower and 'rÃ©sumÃ©' in feedback_lower:          # Ligne 17
            neutral_count += 1                                                  # Ligne 18
        
        # DÃ©cision finale basÃ©e sur les comptages
        if positive_count > negative_count and positive_count > neutral_count:  # Ligne 19
            polarity = "positive"                                               # Ligne 20
        elif negative_count > positive_count and negative_count > neutral_count:  # Ligne 21
            polarity = "nÃ©gative"                                               # Ligne 22
        elif neutral_count > 0:                                                 # Ligne 23
            polarity = "neutre"                                                 # Ligne 24
        else:
            polarity = "neutre"                                                 # Ligne 25
        
        logger.debug(f"Analyse: pos={positive_count}, neg={negative_count}, neu={neutral_count} -> {polarity}")  # Ligne 26
        return {"aspect": "Contenu pÃ©dagogique", "polarity": polarity}          # Ligne 27

    def analyze_sentiment_textblob(self, feedback_text):
        """
        Analyse de sentiment avec TextBlob (mÃ©thode de backup).
        UtilisÃ©e si les rÃ¨gles ne donnent pas de rÃ©sultat satisfaisant.
        """
        try:
            blob = TextBlob(feedback_text)                                      # Ligne 1
            sentiment_score = blob.sentiment.polarity                          # Ligne 2
            
            # VÃ©rification du contenu pÃ©dagogique
            content_keywords = ['contenu', 'cours', 'rÃ©ponse', 'explication', 'rÃ©sumÃ©']  # Ligne 3
            has_content = any(keyword in feedback_text.lower()                  # Ligne 4
                            for keyword in content_keywords)                    # Ligne 5
            
            if not has_content:                                                 # Ligne 6
                return {"aspect": "Contenu pÃ©dagogique", "polarity": "non mentionnÃ©"}  # Ligne 7
            
            # Classification selon le score
            if sentiment_score > 0.1:                                           # Ligne 8
                polarity = "positive"                                           # Ligne 9
            elif sentiment_score < -0.1:                                        # Ligne 10
                polarity = "nÃ©gative"                                           # Ligne 11
            else:
                polarity = "neutre"                                             # Ligne 12
            
            return {"aspect": "Contenu pÃ©dagogique", "polarity": polarity}      # Ligne 13
            
        except:
            return self.analyze_sentiment_rule_based(feedback_text)             # Ligne 14

    def analyze_sentiment_llm(self, feedback_text):
        """
        Analyse avec LLM externe (Groq/Ollama) si disponible.
        MÃ©thode la plus sophistiquÃ©e mais optionnelle.
        """
        if not self.llm:                                                        # Ligne 1
            return None                                                         # Ligne 2
        
        try:
            prompt = f"""Analyse le sentiment de ce feedback concernant le contenu pÃ©dagogique.

Feedback: "{feedback_text}"

RÃ©ponds UNIQUEMENT avec un JSON valide:
{{"aspect": "Contenu pÃ©dagogique", "polarity": "positive/nÃ©gative/neutre/non mentionnÃ©"}}

Si le feedback ne mentionne pas le contenu pÃ©dagogique, utilise "non mentionnÃ©".
"""                                                                             # Ligne 3
            
            ollama_api = OllamaAPI()                                            # Ligne 4
            response = ollama_api.chat_with_ollama(prompt)                      # Ligne 5
            response_text = response.content if hasattr(response, 'content') else str(response)  # Ligne 6
            
            # Extraction du JSON de la rÃ©ponse
            start_idx = response_text.find('{')                                 # Ligne 7
            end_idx = response_text.rfind('}')                                  # Ligne 8
            
            if start_idx != -1 and end_idx != -1:                              # Ligne 9
                json_str = response_text[start_idx:end_idx+1]                   # Ligne 10
                result = json.loads(json_str)                                   # Ligne 11
                
                if 'aspect' in result and 'polarity' in result:                 # Ligne 12
                    return result                                               # Ligne 13
        
        except Exception as e:
            logger.warning(f"Erreur LLM: {e}")                                  # Ligne 14
        
        return None                                                             # Ligne 15

    def analyze_single_feedback(self, feedback_text):
        """
        Analyse un feedback avec cascade de mÃ©thodes (LLM â†’ RÃ¨gles â†’ TextBlob).
        
        Ligne 1-2: VÃ©rification que le feedback n'est pas vide
        Ligne 3: Nettoyage et limitation Ã  500 caractÃ¨res
        Lignes 4-8: Tentative d'analyse avec LLM
        Ligne 9: Fallback sur analyse par rÃ¨gles
        """
        if not feedback_text or not feedback_text.strip():                     # Ligne 1
            return {"aspect": "Contenu pÃ©dagogique", "polarity": "vide"}        # Ligne 2
        
        cleaned_feedback = feedback_text.strip()[:500]                         # Ligne 3
        
        # MÃ©thode 1: LLM (si disponible)
        if self.llm:                                                            # Ligne 4
            llm_result = self.analyze_sentiment_llm(cleaned_feedback)           # Ligne 5
            if llm_result:                                                      # Ligne 6
                logger.info(f"Analyse LLM rÃ©ussie: {llm_result}")               # Ligne 7
                return llm_result                                               # Ligne 8
        
        # MÃ©thode 2: RÃ¨gles (principale et plus fiable)
        rule_result = self.analyze_sentiment_rule_based(cleaned_feedback)       # Ligne 9
        logger.info(f"Analyse par rÃ¨gles: {rule_result}")                       # Ligne 10
        return rule_result                                                      # Ligne 11
```

### 4.3 Explication DÃ©taillÃ©e : Architecture de l'Analyse

**Initialisation (Lignes 1-12) :**
- Configuration optionnelle du LLM Groq pour analyse avancÃ©e
- `temperature=0` : rÃ©ponses dÃ©terministes (pas de crÃ©ativitÃ©)
- `max_tokens=200` : limitation pour Ã©viter les rÃ©ponses trop longues
- Gestion gracieuse si l'API n'est pas disponible

**Analyse par RÃ¨gles (Lignes 1-27) :**
- **Ligne 1** : Conversion en minuscules pour uniformiser
- **Lignes 2-5** : DÃ©finition de 4 catÃ©gories de mots-clÃ©s
- **Ligne 6** : VÃ©rification que le feedback parle bien de contenu pÃ©dagogique
- **Lignes 9-14** : Comptage des occurrences dans chaque catÃ©gorie
- **Lignes 15-18** : Gestion de cas spÃ©ciaux ("trop long", prÃ©fÃ©rences)
- **Lignes 19-25** : Logique de dÃ©cision par majoritÃ©

**Analyse TextBlob (Lignes 1-14) :**
- **Ligne 1** : CrÃ©ation de l'objet TextBlob
- **Ligne 2** : Score de -1 (trÃ¨s nÃ©gatif) Ã  +1 (trÃ¨s positif)
- **Lignes 8-12** : Classification avec seuils (Â±0.1)
- **Ligne 14** : Fallback sur rÃ¨gles en cas d'erreur

**Analyse LLM (Lignes 1-15) :**
- **Ligne 3** : Prompt structurÃ© demandant une rÃ©ponse JSON
- **Lignes 4-6** : Appel au modÃ¨le via Ollama
- **Lignes 7-13** : Extraction et validation du JSON retournÃ©

### 4.4 Code Complet : Gestion des DonnÃ©es et Traitement en Lot

ğŸ“ **Fichier : `sentiment_analyzer.py` (mÃ©thodes de traitement)**

```python
def fetch_feedbacks(self):
    """
    RÃ©cupÃ¨re les feedbacks non analysÃ©s depuis la base de donnÃ©es.
    
    Ligne 1: Connexion Ã  la base SQLite
    Ligne 2-4: ExÃ©cution de la requÃªte pour les feedbacks sans polaritÃ©
    Ligne 5: Log du nombre de feedbacks rÃ©cupÃ©rÃ©s
    """
    engine = create_engine(DATABASE_URL)                                   # Ligne 1
    try:
        with engine.connect() as connection:                               # Ligne 2
            df = pd.read_sql_query(SQL_QUERY, connection)                  # Ligne 3
        logger.info(f"{len(df)} feedbacks Ã  traiter rÃ©cupÃ©rÃ©s")            # Ligne 4
        return df                                                          # Ligne 5
    except Exception as e:
        logger.error(f"Erreur base de donnÃ©es: {e}")                       # Ligne 6
        return pd.DataFrame()                                              # Ligne 7

def update_feedback_polarity(self, feedback_id, polarity):
    """
    Met Ã  jour la polaritÃ© d'un feedback en base de donnÃ©es.
    
    Ligne 1: Connexion avec commit automatique
    Lignes 2-5: ExÃ©cution de la mise Ã  jour
    Ligne 6: Log de confirmation
    """
    engine = create_engine(DATABASE_URL)                                   # Ligne 1
    try:
        with engine.begin() as connection:                                 # Ligne 2
            connection.execute(                                            # Ligne 3
                text("UPDATE feedbacks SET polarity = :polarity WHERE id = :id"),  # Ligne 4
                {"polarity": polarity, "id": feedback_id}                  # Ligne 5
            )
        logger.info(f"Feedback ID {feedback_id} mis Ã  jour avec '{polarity}'")  # Ligne 6
    except Exception as e:
        logger.error(f"Erreur mise Ã  jour feedback ID {feedback_id}: {e}")  # Ligne 7

def analyze_and_update_feedbacks(self):
    """
    Traite tous les feedbacks non analysÃ©s et met Ã  jour la base.
    MÃ©thode principale pour l'analyse en lot.
    """
    feedbacks_df = self.fetch_feedbacks()                                  # Ligne 1
    if feedbacks_df.empty:                                                 # Ligne 2
        logger.warning("Aucun feedback Ã  traiter")                         # Ligne 3
        return self.reload_feedbacks_for_dashboard()                       # Ligne 4
    
    total_feedbacks = len(feedbacks_df)                                    # Ligne 5
    logger.info(f"DÃ©but de l'analyse de {total_feedbacks} feedbacks")      # Ligne 6
    
    # Traitement de chaque feedback
    for idx, row in feedbacks_df.iterrows():                              # Ligne 7
        logger.info(f"{'='*50}")                                           # Ligne 8
        logger.info(f"Analyse du feedback nÂ°{idx + 1}/{total_feedbacks} de {row['User']}")  # Ligne 9
        
        # VÃ©rification du contenu
        if pd.isna(row['FeedBack']) or not row['FeedBack'].strip():         # Ligne 10
            sentiment = {"aspect": "Contenu pÃ©dagogique", "polarity": "vide"}  # Ligne 11
        else:
            feedback = row['FeedBack']                                     # Ligne 12
            logger.info(f"Feedback: '{feedback[:100]}{'...' if len(feedback) > 100 else ''}'")  # Ligne 13
            sentiment = self.analyze_single_feedback(feedback)             # Ligne 14
        
        # Mise Ã  jour en base
        self.update_feedback_polarity(row['ID'], sentiment["polarity"])    # Ligne 15
        logger.info(f"-> RÃ©sultat: {sentiment}")                           # Ligne 16
        
        # Pause pour Ã©viter la surcharge
        time.sleep(0.1)                                                    # Ligne 17
    
    # Rechargement pour le dashboard
    return self.reload_feedbacks_for_dashboard()                          # Ligne 18

def reload_feedbacks_for_dashboard(self):
    """
    Recharge tous les feedbacks avec leur polaritÃ© pour affichage dashboard.
    
    Lignes 1-9: RequÃªte pour rÃ©cupÃ©rer tous les feedbacks analysÃ©s
    Ligne 10: Remplacement des valeurs nulles par 'non mentionnÃ©'
    """
    engine = create_engine(DATABASE_URL)                                   # Ligne 1
    with engine.connect() as connection:                                   # Ligne 2
        analyzed_df = pd.read_sql_query(                                   # Ligne 3
            """
            SELECT "ID", DÃ©partement, FiliÃ¨re, Profile, "User", polarity, "timestamp"
            FROM "V_FeedBack"
            """,                                                           # Ligne 4
            connection                                                     # Ligne 5
        )
    analyzed_df['polarity'] = analyzed_df['polarity'].fillna('non mentionnÃ©')  # Ligne 6
    return analyzed_df                                                     # Ligne 7
```

### 4.5 Explication DÃ©taillÃ©e : Traitement en Lot

**RÃ©cupÃ©ration des donnÃ©es (Lignes 1-7) :**
- `DATABASE_URL` pointe vers la base SQLite locale
- `SQL_QUERY` sÃ©lectionne uniquement les feedbacks sans polaritÃ© (`WHERE polarity IS NULL`)
- Gestion d'erreur avec retour DataFrame vide si problÃ¨me

**Mise Ã  jour individuelle (Lignes 1-7) :**
- `engine.begin()` dÃ©marre une transaction avec commit automatique
- `text()` permet d'utiliser du SQL paramÃ©trÃ© sÃ©curisÃ©
- Ã‰vite les injections SQL avec les paramÃ¨tres liÃ©s

**Traitement en lot (Lignes 1-18) :**
- **Ligne 4** : Si pas de feedbacks, recharge quand mÃªme pour le dashboard
- **Lignes 7-9** : Boucle avec affichage du progrÃ¨s
- **Lignes 10-14** : Gestion des feedbacks vides vs avec contenu
- **Ligne 17** : Pause de 0.1s pour Ã©viter la surcharge systÃ¨me

### 4.6 Exemples Pratiques d'Analyse

**Exemple 1 : Feedback Positif**
```
Input: "Le cours sur l'IA Ã©tait vraiment excellent, trÃ¨s clair et bien expliquÃ© !"

Analyse par rÃ¨gles:
- content_keywords trouvÃ©s: ['cours'] â†’ has_content = True
- positive_keywords trouvÃ©s: ['excellent', 'trÃ¨s clair', 'bien expliquÃ©'] â†’ positive_count = 3
- negative_keywords trouvÃ©s: [] â†’ negative_count = 0
- neutral_keywords trouvÃ©s: [] â†’ neutral_count = 0

RÃ©sultat: {"aspect": "Contenu pÃ©dagogique", "polarity": "positive"}
```

**Exemple 2 : Feedback NÃ©gatif**
```
Input: "La rÃ©ponse Ã©tait confuse et peu claire, difficile Ã  comprendre"

Analyse par rÃ¨gles:
- content_keywords trouvÃ©s: ['rÃ©ponse'] â†’ has_content = True
- positive_keywords trouvÃ©s: [] â†’ positive_count = 0
- negative_keywords trouvÃ©s: ['confuse', 'peu claire', 'difficile'] â†’ negative_count = 3
- neutral_keywords trouvÃ©s: [] â†’ neutral_count = 0

RÃ©sultat: {"aspect": "Contenu pÃ©dagogique", "polarity": "nÃ©gative"}
```

**Exemple 3 : Feedback Hors Sujet**
```
Input: "Bonjour, comment allez-vous aujourd'hui ?"

Analyse par rÃ¨gles:
- content_keywords trouvÃ©s: [] â†’ has_content = False

RÃ©sultat: {"aspect": "Contenu pÃ©dagogique", "polarity": "non mentionnÃ©"}
```

**Exemple 4 : Feedback avec LLM**
```
Input: "Le contenu Ã©tait correct mais manquait d'exemples pratiques"

LLM Response: {"aspect": "Contenu pÃ©dagogique", "polarity": "neutre"}

Justification: MÃ©lange de satisfaction ("correct") et de critique constructive ("manquait d'exemples")
```

---

## 5. IntÃ©gration et Workflow Complet {#workflow}

### 5.1 Flux Utilisateur Type

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Ã‰tudiant     â”‚
â”‚ pose question   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Authentif.   â”‚ â”€â”€â”€â”€ DÃ©partement/FiliÃ¨re
â”‚ & Filtrage      â”‚      dÃ©terminÃ©s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. RAG Search   â”‚ â”€â”€â”€â”€ Recherche dans
â”‚ (avec MMR)      â”‚      documents autorisÃ©s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. GÃ©nÃ©ration   â”‚ â”€â”€â”€â”€ Prompt adaptÃ©
â”‚ LLM Response    â”‚      (standard/QCM/rÃ©sumÃ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. RÃ©ponse      â”‚ â”€â”€â”€â”€ Sauvegarde en
â”‚ + Historique    â”‚      historique
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Feedback     â”‚ â”€â”€â”€â”€ Analyse sentiment
â”‚ (optionnel)     â”‚      pour amÃ©lioration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Code Complet : Point d'EntrÃ©e Principal

ğŸ“ **Fichier : `main.py` (point d'entrÃ©e de l'application)**

```python
import os
from ollama_api import OllamaAPI
from rag_chatbot import RAGChatbot

def main():
    """
    Point d'entrÃ©e principal du systÃ¨me RAG.
    GÃ¨re l'interface utilisateur et orchestre les opÃ©rations.
    """
    # Initialisation des composants principaux
    ollama_api = OllamaAPI()                                                # Ligne 1
    chatbot = RAGChatbot(ollama_api)                                        # Ligne 2

    # Affichage du menu principal
    print("Bienvenue dans le systÃ¨me RAG !")                               # Ligne 3
    print("Commandes disponibles :")                                       # Ligne 4
    print("- 'index chemin_du_fichier' : pour indexer un fichier")         # Ligne 5
    print("- 'chat' : pour dÃ©marrer une conversation")                     # Ligne 6
    print("- 'exit' : pour quitter")                                       # Ligne 7

    # Boucle principale d'interaction
    while True:                                                             # Ligne 8
        command = input("\nEntrez une commande : ").strip()                # Ligne 9

        if command.lower() == "exit":                                       # Ligne 10
            print("Au revoir !")                                           # Ligne 11
            break                                                           # Ligne 12
            
        elif command.lower() == "chat":                                     # Ligne 13
            try:
                # Configuration du contexte acadÃ©mique
                print("=== Configuration du contexte acadÃ©mique ===")       # Ligne 14
                departement_id = int(input("ID du dÃ©partement : "))         # Ligne 15
                filiere_id = int(input("ID de la filiÃ¨re : "))              # Ligne 16
                module_id = int(input("ID du module : "))                   # Ligne 17
                activite_id = int(input("ID de l'activitÃ© : "))             # Ligne 18
                profile_id = int(input("ID du profil (1=Admin, 2=Prof, 3=Etudiant, 4=InvitÃ©) : "))  # Ligne 19
                user_id = int(input("Votre identifiant utilisateur : "))    # Ligne 20
                print("Contexte enregistrÃ©.\n")                            # Ligne 21

                # DÃ©marrage de la conversation
                chatbot.chat(                                               # Ligne 22
                    departement_id=departement_id,                         # Ligne 23
                    filiere_id=filiere_id,                                  # Ligne 24
                    module_id=module_id,                                    # Ligne 25
                    activite_id=activite_id,                                # Ligne 26
                    profile_id=profile_id,                                  # Ligne 27
                    user_id=user_id                                         # Ligne 28
                )

            except ValueError:
                print("Erreur : tous les identifiants doivent Ãªtre des entiers valides.")  # Ligne 29
                
        elif command.lower().startswith("index "):                         # Ligne 30
            file_path = command[6:].strip()                                 # Ligne 31
            base_filename = os.path.basename(file_path)                     # Ligne 32
            
            if not os.path.exists(file_path):                              # Ligne 33
                print(f"Le fichier {file_path} n'existe pas.")             # Ligne 34
                continue                                                    # Ligne 35
            
            try:
                # Collecte des mÃ©tadonnÃ©es du fichier
                departement_id = int(input("ID du dÃ©partement : "))         # Ligne 36
                filiere_id = int(input("ID de la filiÃ¨re : "))              # Ligne 37
                module_id = int(input("ID du module : "))                   # Ligne 38
                activite_id = int(input("ID de l'activitÃ© (1=Inscription, 2=Cours, 3=TD, 4=TP...) : "))  # Ligne 39
                profile_id = int(input("ID du profil (1=Admin, 2=Prof, 3=Etudiant, 4=InvitÃ©) : "))  # Ligne 40
                user_id = int(input("ID de l'utilisateur : "))              # Ligne 41

                # Indexation du fichier
                chatbot.ingestion_file(                                     # Ligne 42
                    base_filename,                                          # Ligne 43
                    file_path,                                              # Ligne 44
                    departement_id=departement_id,                         # Ligne 45
                    filiere_id=filiere_id,                                  # Ligne 46
                    module_id=module_id,                                    # Ligne 47
                    activite_id=activite_id,                                # Ligne 48
                    profile_id=profile_id,                                  # Ligne 49
                    user_id=user_id                                         # Ligne 50
                )

            except ValueError:
                print("Erreur : tous les identifiants doivent Ãªtre des entiers valides.")  # Ligne 51
            except Exception as e:
                print(f"Erreur lors de l'indexation : {e}")                # Ligne 52
                
        else:
            print("Commande non reconnue. Veuillez rÃ©essayer.")            # Ligne 53

if __name__ == "__main__":
    main()                                                                  # Ligne 54
```

### 5.3 Explication DÃ©taillÃ©e : Interface Principale

**Lignes 1-2 : Initialisation**
- `OllamaAPI()` : Interface avec le modÃ¨le de langage local
- `RAGChatbot()` : Orchestrateur principal du systÃ¨me RAG

**Lignes 3-7 : Interface utilisateur**
- Menu simple et clair pour l'utilisateur
- Trois commandes principales : index, chat, exit

**Lignes 8-12 : Boucle principale**
- `while True` : fonctionnement continu jusqu'Ã  'exit'
- `input().strip()` : rÃ©cupÃ©ration et nettoyage des commandes

**Lignes 13-29 : Mode conversation**
- Configuration complÃ¨te du contexte acadÃ©mique
- Identification de l'utilisateur et de ses permissions
- Gestion d'erreur pour les entrÃ©es invalides

**Lignes 30-53 : Mode indexation**
- Extraction du chemin depuis la commande
- VÃ©rification que le fichier existe
- Collecte des mÃ©tadonnÃ©es pour le contexte acadÃ©mique
- Appel de l'ingestion avec tous les paramÃ¨tres

### 5.4 Architecture ComplÃ¨te du SystÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            SYSTÃˆME RAG COMPLET                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   main.py       â”‚    â”‚  rag_chatbot.py â”‚    â”‚file_processor.pyâ”‚         â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚         â”‚
â”‚  â”‚ â€¢ Interface CLI â”‚â”€â”€â”€â”€â”‚ â€¢ Orchestration â”‚â”€â”€â”€â”€â”‚ â€¢ Traitement    â”‚         â”‚
â”‚  â”‚ â€¢ Gestion users â”‚    â”‚ â€¢ Recherche RAG â”‚    â”‚   fichiers      â”‚         â”‚
â”‚  â”‚ â€¢ Menu commandesâ”‚    â”‚ â€¢ GÃ©nÃ©ration LLMâ”‚    â”‚ â€¢ Chunking      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                       â”‚                       â”‚                â”‚
â”‚           â”‚                       â”‚                       â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ filter_manager. â”‚    â”‚   ollama_api.py â”‚    â”‚sentiment_analyzerâ”‚        â”‚
â”‚  â”‚      py         â”‚    â”‚                 â”‚    â”‚      .py        â”‚         â”‚
â”‚  â”‚                 â”‚    â”‚ â€¢ Interface LLM â”‚    â”‚                 â”‚         â”‚
â”‚  â”‚ â€¢ Filtres       â”‚â”€â”€â”€â”€â”‚ â€¢ Communicationâ”‚â”€â”€â”€â”€â”‚ â€¢ Analyse       â”‚         â”‚
â”‚  â”‚   acadÃ©miques   â”‚    â”‚   modÃ¨le        â”‚    â”‚   feedbacks     â”‚         â”‚
â”‚  â”‚ â€¢ Base SQLite   â”‚    â”‚ â€¢ Prompts       â”‚    â”‚ â€¢ 3 mÃ©thodes    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                       â”‚                       â”‚                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                   â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         COMPOSANTS EXTERNES                         â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚    FAISS    â”‚  â”‚   SQLite    â”‚  â”‚   Ollama    â”‚  â”‚  SentenceT. â”‚ â”‚   â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Index     â”‚  â”‚ â€¢ Metadata  â”‚  â”‚ â€¢ LLM Local â”‚  â”‚ â€¢ Embeddingsâ”‚ â”‚   â”‚
â”‚  â”‚  â”‚   vectoriel â”‚  â”‚ â€¢ History   â”‚  â”‚ â€¢ GÃ©nÃ©rationâ”‚  â”‚ â€¢ BGE Model â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Recherche â”‚  â”‚ â€¢ Users     â”‚  â”‚   texte     â”‚  â”‚ â€¢ Vectors   â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.5 Points ClÃ©s pour l'Optimisation

1. **QualitÃ© des Embeddings** : Le modÃ¨le BGE-base-en-v1.5 est optimisÃ© pour la langue franÃ§aise
2. **Taille des Chunks** : 384 caractÃ¨res avec 96 de chevauchement = bon Ã©quilibre
3. **MMR Lambda** : 0.5 offre un bon compromis pertinence/diversitÃ©
4. **Seuil de SimilaritÃ©** : 0.65 filtre les rÃ©sultats peu pertinents
5. **Prompts StructurÃ©s** : Garantissent la qualitÃ© et la cohÃ©rence des rÃ©ponses

### 5.6 Monitoring et MÃ©triques

**Performance RAG :**
- Temps de recherche vectorielle
- Nombre de chunks rÃ©cupÃ©rÃ©s
- Scores de similaritÃ© moyens
- Utilisation du MMR (diversitÃ© vs pertinence)

**Analyse de Sentiment :**
- Distribution des polaritÃ©s (positive/nÃ©gative/neutre)
- Taux de feedbacks "non mentionnÃ©"
- Performance des diffÃ©rentes mÃ©thodes d'analyse
- Ã‰volution temporelle de la satisfaction

**Utilisation SystÃ¨me :**
- Nombre de documents indexÃ©s
- Volume de l'index FAISS
- FrÃ©quence des requÃªtes par utilisateur
- RÃ©partition par dÃ©partement/filiÃ¨re

---

## Conclusion

Ce systÃ¨me RAG avec analyse de sentiment reprÃ©sente une solution complÃ¨te pour l'Ã©ducation numÃ©rique :

âœ… **RÃ©ponses prÃ©cises** grÃ¢ce Ã  la recherche vectorielle FAISS  
âœ… **DiversitÃ© des contenus** avec l'algorithme MMR  
âœ… **Personnalisation** par profil utilisateur et filtrage acadÃ©mique  
âœ… **AmÃ©lioration continue** via l'analyse des feedbacks  
âœ… **SÃ©curitÃ©** et contrÃ´le d'accÃ¨s par dÃ©partement/filiÃ¨re  
âœ… **Architecture modulaire** permettant l'Ã©volution et la maintenance  

L'architecture modulaire permet facilement d'ajouter de nouvelles fonctionnalitÃ©s ou d'amÃ©liorer les composants existants sans affecter le reste du systÃ¨me.

#### ğŸ” **Recherche de SimilaritÃ© Standard**

```python
query_embedding = self.embedding_model.encode([user_query], normalize_embeddings=True)[0]
distances, indices = self.index.search(normalized_query, k=5)
```

Le systÃ¨me :
1. **Convertit** la question en vecteur
2. **Cherche** les vecteurs les plus proches dans FAISS
3. **RÃ©cupÃ¨re** les chunks correspondants

#### ğŸ¯ **MMR (Maximal Marginal Relevance)**

**Le problÃ¨me :** La recherche standard peut retourner des passages trÃ¨s similaires (redondants).

**La solution MMR :**

```python
def mmr(self, query_embedding, candidate_embeddings, k=3, lambda_param=0.5):
    selected = []
    for _ in range(k):
        max_score = -np.inf
        for idx in candidate_indices:
            # Pertinence par rapport Ã  la question
            relevance = similarity(query, candidate[idx])
            
            # DiversitÃ© par rapport aux dÃ©jÃ  sÃ©lectionnÃ©s  
            diversity = max([similarity(candidate[idx], selected_doc) for selected_doc in selected])
            
            # Score MMR = Ã©quilibre pertinence/diversitÃ©
            mmr_score = lambda_param * relevance - (1 - lambda_param) * diversity
            
            if mmr_score > max_score:
                best_candidate = idx
```

**Comment MMR fonctionne :**

1. **Pertinence** : Ã€ quel point ce passage rÃ©pond Ã  la question ?
2. **DiversitÃ©** : Ã€ quel point ce passage apporte des informations nouvelles ?
3. **Ã‰quilibre** : `lambda_param` contrÃ´le le compromis
   - `lambda = 1.0` : PrioritÃ© totale Ã  la pertinence
   - `lambda = 0.0` : PrioritÃ© totale Ã  la diversitÃ©
   - `lambda = 0.5` : Ã‰quilibre

**Exemple pratique :**

Question : "Qu'est-ce que le machine learning ?"

Sans MMR :
- Passage 1 : "Le machine learning est une branche de l'IA..."
- Passage 2 : "Le machine learning, ou apprentissage automatique..."
- Passage 3 : "Le ML fait partie de l'intelligence artificielle..."

Avec MMR :
- Passage 1 : "Le machine learning est une branche de l'IA..."
- Passage 2 : "Les algorithmes supervisÃ©s utilisent des donnÃ©es Ã©tiquetÃ©es..."
- Passage 3 : "Les applications incluent la reconnaissance vocale..."

#### ğŸ”„ **Filtrage AcadÃ©mique**

```python
allowed_faiss_ids = FilterManager.get_allowed_indices(departement_id, filiere_id)
```

Le systÃ¨me s'assure que l'Ã©tudiant accÃ¨de **uniquement** aux documents de :
- Son **dÃ©partement** (ex: Informatique, MathÃ©matiques)
- Sa **filiÃ¨re** (ex: GÃ©nie Logiciel, Data Science)
- Ses **modules** et **activitÃ©s**

### 3.4 GÃ©nÃ©ration de la RÃ©ponse

#### ğŸ“ **Construction du Prompt**

```python
if PromptBuilder.is_qcm_request(user_query):
    prompt_text = PromptBuilder.build_qcm_prompt(context_text, user_query)
elif "rÃ©sumÃ©" in user_query.lower():
    prompt_text = PromptBuilder.build_summary_prompt(context_text, user_query)
else:
    prompt_text = PromptBuilder.build_standard_prompt(context_text, user_query)
```

Le systÃ¨me adapte son comportement selon le type de demande :

**Prompt Standard :**
```
# Contexte de l'Apprentissage
[Passages pertinents trouvÃ©s]

# Ta Mission
Tu es un assistant pÃ©dagogique expert...

# Question de l'Ã‰tudiant
[Question de l'utilisateur]

## Tes Directives :
1. Source unique : RÃ©ponds uniquement Ã  partir du contexte
2. Ton engageant et motivant
3. RÃ©ponse concise et claire
4. Uniquement en franÃ§ais
```

#### ğŸ¤– **Appel au LLM (Ollama)**

```python
llm_raw_response = self.ollama_api.chat_with_ollama(prompt_text)
cleaned_response = self.clean_llm_response(llm_raw_response)
```

Le **Large Language Model** (via Ollama) :
- Analyse le contexte fourni
- GÃ©nÃ¨re une rÃ©ponse adaptÃ©e
- Respecte les consignes pÃ©dagogiques

---

## 4. Analyse de Sentiment {#sentiment}

### 4.1 Objectif de l'Analyse de Sentiment

L'analyse de sentiment permet de :
- **Comprendre** la satisfaction des Ã©tudiants
- **Identifier** les points d'amÃ©lioration
- **Adapter** le systÃ¨me selon les retours

### 4.2 Architecture du SystÃ¨me d'Analyse

```python
class SimpleSentimentAnalyzer:
    def __init__(self):
        self.llm = ChatGroq(model_name="llama3-8b-8192")  # LLM externe (optionnel)
```

### 4.3 MÃ©thodes d'Analyse (en Cascade)

#### ğŸ¯ **MÃ©thode 1 : Analyse par LLM (Prioritaire)**

```python
def analyze_sentiment_llm(self, feedback_text):
    prompt = f"""Analyse le sentiment de ce feedback concernant le contenu pÃ©dagogique.
    
    Feedback: "{feedback_text}"
    
    RÃ©ponds UNIQUEMENT avec un JSON valide:
    {{"aspect": "Contenu pÃ©dagogique", "polarity": "positive/nÃ©gative/neutre/non mentionnÃ©"}}
    """
    response = ollama_api.chat_with_ollama(prompt)
```

**Avantages :**
- **ComprÃ©hension contextuelle** avancÃ©e
- **Nuances** linguistiques
- **AdaptabilitÃ©** aux expressions variÃ©es

#### ğŸ“Š **MÃ©thode 2 : Analyse par RÃ¨gles (Fallback principal)**

```python
def analyze_sentiment_rule_based(self, feedback_text):
    feedback_lower = feedback_text.lower()
    
    # DÃ©tection du contenu pÃ©dagogique
    content_keywords = [
        'contenu', 'cours', 'leÃ§on', 'rÃ©sumÃ©', 'explication', 'pÃ©dagogique',
        'matiÃ¨re', 'sujet', 'chapitre', 'module', 'formation', 'apprentissage'
    ]
    
    # Mots positifs
    positive_keywords = [
        'gÃ©nial', 'super', 'magnifique', 'excellent', 'parfait', 'bien', 'bon',
        'efficace', 'utile', 'clair', 'prÃ©cis', 'complet', 'satisfait',
        'trÃ¨s utile', 'trÃ¨s clair', 'bien expliquÃ©'
    ]
    
    # Mots nÃ©gatifs
    negative_keywords = [
        'mauvais', 'nul', 'horrible', 'dÃ©cevant', 'problÃ¨me', 'erreur',
        'difficile', 'compliquÃ©', 'confus', 'incomprÃ©hensible',
        'peu clair', 'pas utile', 'pas efficace'
    ]
```

**Algorithme :**

1. **VÃ©rification de pertinence :**
   ```python
   has_content = any(keyword in feedback_lower for keyword in content_keywords)
   if not has_content:
       return {"aspect": "Contenu pÃ©dagogique", "polarity": "non mentionnÃ©"}
   ```

2. **Comptage des occurrences :**
   ```python
   positive_count = sum(1 for keyword in positive_keywords if keyword in feedback_lower)
   negative_count = sum(1 for keyword in negative_keywords if keyword in feedback_lower)
   ```

3. **DÃ©cision finale :**
   ```python
   if positive_count > negative_count:
       polarity = "positive"
   elif negative_count > positive_count:
       polarity = "nÃ©gative" 
   else:
       polarity = "neutre"
   ```

#### ğŸ“ˆ **MÃ©thode 3 : TextBlob (Backup)**

```python
def analyze_sentiment_textblob(self, feedback_text):
    blob = TextBlob(feedback_text)
    sentiment_score = blob.sentiment.polarity  # Entre -1 (nÃ©gatif) et +1 (positif)
    
    if sentiment_score > 0.1:
        polarity = "positive"
    elif sentiment_score < -0.1:
        polarity = "nÃ©gative"
    else:
        polarity = "neutre"
```

### 4.4 Processus Complet d'Analyse

```python
def analyze_single_feedback(self, feedback_text):
    # VÃ©rification et nettoyage
    if not feedback_text or not feedback_text.strip():
        return {"aspect": "Contenu pÃ©dagogique", "polarity": "vide"}
    
    cleaned_feedback = feedback_text.strip()[:500]  # Limitation Ã  500 caractÃ¨res
    
    # MÃ©thode 1 : LLM (si disponible)
    if self.llm:
        llm_result = self.analyze_sentiment_llm(cleaned_feedback)
        if llm_result:
            return llm_result
    
    # MÃ©thode 2 : RÃ¨gles (principale)
    return self.analyze_sentiment_rule_based(cleaned_feedback)
```

### 4.5 Gestion des DonnÃ©es et Mises Ã  Jour

#### ğŸ’¾ **RÃ©cupÃ©ration des Feedbacks**

```python
def fetch_feedbacks(self):
    SQL_QUERY = """
    SELECT id as ID, DÃ©partement, FiliÃ¨re, Profile, "User", FeedBack, "timestamp"
    FROM "V_FeedBack"
    WHERE polarity IS NULL  -- Seulement les feedbacks non analysÃ©s
    """
```

#### ğŸ”„ **Mise Ã  Jour en Base**

```python
def update_feedback_polarity(self, feedback_id, polarity):
    connection.execute(
        text("UPDATE feedbacks SET polarity = :polarity WHERE id = :id"),
        {"polarity": polarity, "id": feedback_id}
    )
```

**Workflow :**
1. RÃ©cupÃ©rer les feedbacks non analysÃ©s
2. Analyser chaque feedback
3. Stocker le rÃ©sultat en base
4. GÃ©nÃ©rer des statistiques pour le dashboard

---

## 5. IntÃ©gration et Workflow Complet {#workflow}

### 5.1 Flux Utilisateur Type

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Ã‰tudiant     â”‚
â”‚ pose question   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Authentif.   â”‚ â”€â”€â”€â”€ DÃ©partement/FiliÃ¨re
â”‚ & Filtrage      â”‚      dÃ©terminÃ©s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. RAG Search   â”‚ â”€â”€â”€â”€ Recherche dans
â”‚ (avec MMR)      â”‚      documents autorisÃ©s
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. GÃ©nÃ©ration   â”‚ â”€â”€â”€â”€ Prompt adaptÃ©
â”‚ LLM Response    â”‚      (standard/QCM/rÃ©sumÃ©)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. RÃ©ponse      â”‚ â”€â”€â”€â”€ Sauvegarde en
â”‚ + Historique    â”‚      historique
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Feedback     â”‚ â”€â”€â”€â”€ Analyse sentiment
â”‚ (optionnel)     â”‚      pour amÃ©lioration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Gestion des Profils Utilisateur

```python
INVITE_PROFILE_ID = 5

if profile_id == INVITE_PROFILE_ID:
    # Restrictions spÃ©ciales pour les invitÃ©s
    departement_id = 1  # Seulement ScolaritÃ©
    filiere_id = 3
    
    prompt_text = (
        "âš ï¸ Mode invitÃ© activÃ© :\n"
        "Tu dois rÃ©pondre uniquement Ã  partir des informations du dÃ©partement ScolaritÃ©.\n"
        "Si la question ne concerne pas le dÃ©partement ScolaritÃ©, rÃ©ponds :\n"
        "'Je ne peux rÃ©pondre qu'aux questions relatives au dÃ©partement ScolaritÃ©.'"
    )
```

### 5.3 Optimisations et Bonnes Pratiques

#### âš¡ **Performance**
- **Cache FAISS** : Les index sont sauvegardÃ©s sur disque
- **Hashes de fichiers** : Ã‰vitent le retraitement des documents identiques
- **Pagination** : Limitation des rÃ©sultats pour les grosses requÃªtes

#### ğŸ”’ **SÃ©curitÃ©**
- **Filtrage strict** par dÃ©partement/filiÃ¨re
- **Validation des inputs** utilisateur
- **Hachage des mots de passe**
- **Sanitization** des rÃ©ponses LLM

#### ğŸ“Š **Monitoring**
- **Logging dÃ©taillÃ©** de toutes les opÃ©rations
- **MÃ©triques de performance** (temps de rÃ©ponse, prÃ©cision)
- **Dashboard d'analyse** des feedbacks

### 5.4 Points ClÃ©s pour l'Optimisation

1. **QualitÃ© des Embeddings** : Le modÃ¨le BGE-base-en-v1.5 est optimisÃ© pour la langue franÃ§aise
2. **Taille des Chunks** : 384 caractÃ¨res avec 96 de chevauchement = bon Ã©quilibre
3. **MMR Lambda** : 0.5 offre un bon compromis pertinence/diversitÃ©
4. **Seuil de SimilaritÃ©** : 0.65 filtre les rÃ©sultats peu pertinents
5. **Prompts StructurÃ©s** : Garantissent la qualitÃ© et la cohÃ©rence des rÃ©ponses

---

## Conclusion

Ce systÃ¨me RAG avec analyse de sentiment reprÃ©sente une solution complÃ¨te pour l'Ã©ducation numÃ©rique :

âœ… **RÃ©ponses prÃ©cises** grÃ¢ce Ã  la recherche vectorielle  
âœ… **DiversitÃ© des contenus** avec MMR  
âœ… **Personnalisation** par profil utilisateur  
âœ… **AmÃ©lioration continue** via l'analyse des feedbacks  
âœ… **SÃ©curitÃ©** et filtrage acadÃ©mique  

L'architecture modulaire permet facilement d'ajouter de nouvelles fonctionnalitÃ©s ou d'amÃ©liorer les composants existants.